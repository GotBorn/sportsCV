{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Запуск с тестовыми данными.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUwzkb-OxNDZ"
      },
      "source": [
        "T4 - хорошо\n",
        "\n",
        "K80 - плохо"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmgXGWpKOjJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9452b66-576b-4199-d85e-a71d0b9aca71"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Bv6nYZ1lGU"
      },
      "source": [
        "Много импортов и определение функции infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIEw46f3wYeY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import csv\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import tqdm\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations\n",
        "from albumentations import pytorch as AT"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_Wot_2wgeq"
      },
      "source": [
        "def SeedEverything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBkY5cDDwGUe"
      },
      "source": [
        "def infer(modelFolderDataPath, datasetPath):\n",
        "    SeedEverything(41)\n",
        "\n",
        "    labelList = ['baseball', 'formula1', 'fencing', 'motogp', 'ice_hockey',# был ранее получен функцией GetCategoryList\n",
        "                 'wrestling', 'boxing', 'volleyball', 'cricket', 'basketball', 'wwe',\n",
        "                 'swimming', 'weight_lifting', 'gymnastics', 'tennis', 'kabaddi', 'badminton',\n",
        "                 'football', 'table_tennis', 'hockey', 'shooting', 'chess']\n",
        "\n",
        "    test_files = os.listdir(datasetPath)\n",
        "\n",
        "    #сортировка файлов в правильном порядке\n",
        "    for i in range(0, len(test_files)):\n",
        "        test_files[i] = int(test_files[i].replace(\".jpg\", \"\"))\n",
        "        \n",
        "    test_files.sort()\n",
        "    \n",
        "    for i in range(0, len(test_files)):\n",
        "        test_files[i] = str(test_files[i]) + \".jpg\"\n",
        "\n",
        "    print(\"Test set size: \", len(test_files))  # 1645\n",
        "    class SportsDataset(Dataset):\n",
        "        def __init__(self, file_list, dir, transform=None):\n",
        "            self.file_list = file_list\n",
        "            self.dir = dir\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.file_list)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image = cv2.imread(os.path.join(self.dir, self.file_list[idx]))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "\n",
        "            return image\n",
        "\n",
        "    img_size = 256\n",
        "\n",
        "    data_transforms_test = albumentations.Compose([\n",
        "        albumentations.Resize(img_size, img_size),\n",
        "        albumentations.Normalize(),\n",
        "        AT.ToTensor()\n",
        "    ])\n",
        "\n",
        "    test_set = SportsDataset(test_files, datasetPath, data_transforms_test)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
        "                                             num_workers=0, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device: \", device)\n",
        "\n",
        "    ##########\n",
        "    modelResnet = torchvision.models.resnet152(pretrained=True, progress=True)\n",
        "    modelResnet.fc = nn.Sequential(\n",
        "      nn.Linear(2048, 1024),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(512, 22)\n",
        "    )\n",
        "    ###########\n",
        "    modelResnext = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n",
        "    modelResnext.fc = nn.Sequential(\n",
        "      nn.Linear(2048, 1500),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(1500, 800),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(800, 300),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(300, 22)\n",
        "    )\n",
        "    ###########\n",
        "    modelGoogleNet = torchvision.models.googlenet(pretrained=True, progress=True)\n",
        "    modelGoogleNet.fc = nn.Sequential(\n",
        "      nn.Linear(1024, 800),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(800, 500),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(500, 200),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(200, 22)\n",
        "    )\n",
        "    ###########\n",
        "    modelDenseNet = torchvision.models.densenet201(pretrained=True, progress=True)\n",
        "    modelDenseNet.classifier = nn.Sequential(\n",
        "      nn.Linear(1920, 1500),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(1500, 1000),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(1000, 400),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(400, 22)\n",
        "    )\n",
        "\n",
        "    modelResnet.load_state_dict(torch.load(modelFolderDataPath + \"/resnet152Model.pt\"))\n",
        "    modelResnext.load_state_dict(torch.load(modelFolderDataPath + \"/ResnextModel.pt\"))\n",
        "    modelGoogleNet.load_state_dict(torch.load(modelFolderDataPath + \"/GoogleNetModel.pt\"))\n",
        "    modelDenseNet.load_state_dict(torch.load(modelFolderDataPath + \"/DenseNetModel.pt\"))\n",
        "\n",
        "    modelResnet.eval()\n",
        "    modelResnext.eval()\n",
        "    modelGoogleNet.eval()\n",
        "    modelDenseNet.eval()\n",
        "    \n",
        "    modelResnet = modelResnet.to(device)\n",
        "    modelResnext = modelResnext.to(device)\n",
        "    modelGoogleNet = modelGoogleNet.to(device)\n",
        "    modelDenseNet = modelDenseNet.to(device)\n",
        "\n",
        "    print(\"Classification started\")\n",
        "\n",
        "    f = open(\"output.csv\", \"w\")\n",
        "    with torch.no_grad():\n",
        "        for i, image in enumerate(testloader, 0):\n",
        "            image = image.to(device=device)\n",
        "\n",
        "            outputResnet = modelResnet(image)\n",
        "            outputResnext = modelResnext(image)\n",
        "            outputGoogleNet = modelGoogleNet(image)\n",
        "            outputDenseNet = modelDenseNet(image)\n",
        "\n",
        "            _, predicted = torch.max((outputResnet.data + outputResnext.data + \n",
        "                              outputGoogleNet.data + outputDenseNet.data) / 4, 1)\n",
        "            sample_fname = testloader.dataset.file_list[i]\n",
        "            line = sample_fname + \",\" + str(labelList[predicted.item()]) + '\\n'\n",
        "            f.write(line)\n",
        "    f.close()\n",
        "    print(\"Classification finished\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2GQRbDaOUx2"
      },
      "source": [
        "Подключение Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qoQyTdPw3UC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4addb0b-1c94-4336-bd8c-ff3c61bba80e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilZ0TgD4SxIL"
      },
      "source": [
        "Я использовал для удобной распаковки датасета в корень коллаба, не обязательно к использованию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGYZdy7bO5Gd"
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/sportsDataset.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')\n",
        "  \n",
        "with zipfile.ZipFile('/content/train.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/train')\n",
        "\n",
        "with zipfile.ZipFile('/content/test.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/test')\n",
        "\n",
        "with zipfile.ZipFile('/content/valid.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/valid')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Zz-gH01zFn"
      },
      "source": [
        "Непосредственно запуск скрипта"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIgLS1OUwE1W"
      },
      "source": [
        "# /content/drive/MyDrive/models\n",
        "# /content/test\n",
        "print(\"Введите полный путь до папки с весами всех 4-х моделей:\")\n",
        "modelFolderDataPath = input()\n",
        "print(\"Введите полный путь до папки датасета (Например D:\\\\dataset):\")\n",
        "datasetPath = input()\n",
        "\n",
        "infer(modelFolderDataPath, datasetPath)\n",
        "\n",
        "print(\"Результат находится в output.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}